{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9686283,"sourceType":"datasetVersion","datasetId":5921282},{"sourceId":9869002,"sourceType":"datasetVersion","datasetId":6057951},{"sourceId":9871041,"sourceType":"datasetVersion","datasetId":6059478}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom transformers import BertModel, BertTokenizer\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/ml-project/train/subtask_a_train.csv')\ntarget_data = pd.read_csv('/kaggle/input/target-files/target_t.csv')\n\n# Image transformations\nimage_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Dataset class\nclass IdiomImageDataset(Dataset):\n    def __init__(self, dataframe, target_df, image_dir):\n        self.dataframe = dataframe\n        self.target_df = target_df\n        self.image_dir = image_dir\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        target_row = self.target_df.iloc[index]\n        sentence = row['sentence']\n        idiom_name = row['compound'].replace(\"'\", \"_\")\n        image_names = [row[f'image{i}_name'] for i in range(1, 6)]\n        \n        expected_order = torch.tensor(eval(target_row['target']), dtype=torch.long) - 1\n\n        # Tokenize text\n        inputs = self.tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        \n        # Load images\n        images = []\n        for img_name in image_names:\n            img_path = os.path.join(self.image_dir, idiom_name, img_name)\n            img = Image.open(img_path).convert('RGB')\n            img = image_transforms(img)\n            images.append(img)\n        images_tensor = torch.stack(images)\n\n        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), images_tensor, expected_order\n\n# Updated model\nclass AdvancedMultimodalRankingModel(nn.Module):\n    def __init__(self):\n        super(AdvancedMultimodalRankingModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.resnet = models.resnet50(weights='DEFAULT')\n        self.resnet.fc = nn.Identity()\n        \n        # Feature projectors\n        self.fc_text = nn.Linear(768, 128)\n        self.fc_image = nn.Linear(2048, 128)\n        \n        # Rank prediction layers\n        self.fc_combined = nn.Sequential(\n            nn.Linear(128*2, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        text_features = text_features.mean(dim=1)\n        text_features = self.fc_text(text_features)\n        \n        batch_size, num_images, channels, height, width = images.size()\n        images = images.view(batch_size * num_images, channels, height, width)\n        image_features = self.resnet(images)\n        image_features = image_features.view(batch_size, num_images, -1)\n        \n        image_features = self.fc_image(image_features)\n        text_features = text_features.unsqueeze(1).repeat(1, num_images, 1)\n        \n        combined_features = torch.cat((text_features, image_features), dim=2)\n        rankings = self.fc_combined(combined_features).squeeze(-1)\n        \n        return rankings\n\n# Set up data loaders and model\nimage_folder = '/kaggle/input/ml-project/train'  \ndataset = IdiomImageDataset(train_data, target_data, image_folder)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Training setup\nmodel = AdvancedMultimodalRankingModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Update the criterion to MSELoss if comparing scores directly\ncriterion = nn.MSELoss()  # Or nn.MarginRankingLoss() for ranking comparisons\n\ndef train_model(model, data_loader, criterion, optimizer, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for input_ids, attention_mask, images, expected_order in data_loader:\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask, images)\n            \n            # If using MSELoss\n            expected_order = expected_order.float()  # Ensure dtype consistency\n            loss = criterion(outputs, expected_order)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data_loader)}')\n\n\n# Run the training process\ntrain_model(model, train_loader, criterion, optimizer, epochs=5)\n\ntorch.save(model.state_dict(), 'advanced_multimodal_ranking_model.pth')\n\n# Evaluation function\ndef evaluate_model(model, data_loader):\n    model.eval()\n    all_predictions = []\n    with torch.no_grad():\n        for input_ids, attention_mask, images, _ in data_loader:\n            outputs = model(input_ids, attention_mask, images)\n            rankings = torch.argsort(outputs, dim=1)\n            all_predictions.extend(rankings.cpu().numpy())\n    return all_predictions\n\npredicted_rankings = evaluate_model(model, test_loader)\npredicted_rankings_final = [[x + 1 for x in ranking] for ranking in predicted_rankings]\npredicted_rankings_final \nprint(predicted_rankings_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:39:01.603798Z","iopub.execute_input":"2024-11-12T18:39:01.604264Z","iopub.status.idle":"2024-11-12T18:47:31.672257Z","shell.execute_reply.started":"2024-11-12T18:39:01.604221Z","shell.execute_reply":"2024-11-12T18:47:31.671137Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 5.19644021987915\nEpoch 2/5, Loss: 2.606148898601532\nEpoch 3/5, Loss: 1.590873658657074\nEpoch 4/5, Loss: 1.031388521194458\nEpoch 5/5, Loss: 0.6192614510655403\n[[3, 4, 5, 1, 2], [5, 4, 3, 2, 1], [1, 4, 5, 3, 2], [2, 5, 4, 3, 1], [4, 1, 5, 2, 3], [4, 1, 2, 3, 5], [3, 4, 1, 5, 2], [1, 5, 4, 3, 2], [1, 2, 3, 4, 5], [5, 3, 4, 2, 1], [2, 3, 1, 4, 5], [1, 2, 5, 4, 3], [3, 2, 4, 1, 5], [5, 4, 1, 2, 3]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Get the indices of the test data points\ntest_indices = test_dataset.indices\n\n# Display the indices\nprint(\"Test Indices:\", test_indices)\n\n# Optionally, print the actual test data points (e.g., true rankings or other features)\ntrue_test_rankings = [eval(target_data.iloc[idx]['target']) for idx in test_indices]\ntrue_test_rankings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:31:25.856468Z","iopub.execute_input":"2024-11-12T18:31:25.856848Z","iopub.status.idle":"2024-11-12T18:31:25.867589Z","shell.execute_reply.started":"2024-11-12T18:31:25.856813Z","shell.execute_reply":"2024-11-12T18:31:25.866682Z"}},"outputs":[{"name":"stdout","text":"Test Indices: [3, 52, 23, 8, 62, 0, 14, 64, 6, 12, 65, 54, 2, 19]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[[4, 3, 1, 2, 5],\n [1, 3, 2, 4, 5],\n [4, 3, 1, 5, 2],\n [2, 3, 4, 5, 1],\n [4, 1, 5, 2, 3],\n [1, 3, 2, 5, 4],\n [3, 1, 2, 4, 5],\n [3, 1, 2, 5, 4],\n [5, 3, 4, 2, 1],\n [5, 3, 4, 1, 2],\n [2, 5, 1, 3, 4],\n [2, 1, 4, 5, 3],\n [3, 2, 1, 5, 4],\n [2, 4, 5, 1, 3]]"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\ndef mean_reciprocal_rank(true_rankings, predicted_rankings):\n    reciprocal_ranks = []\n    for true, pred in zip(true_rankings, predicted_rankings):\n        for i, p in enumerate(pred):\n            if p == true[i]:\n                reciprocal_ranks.append(1 / (i + 1))\n                break\n        else:\n            reciprocal_ranks.append(0)\n    return np.mean(reciprocal_ranks)\nprint(mean_reciprocal_rank(true_test_rankings,predicted_rankings_final ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:48:57.455803Z","iopub.execute_input":"2024-11-12T18:48:57.456333Z","iopub.status.idle":"2024-11-12T18:48:57.464150Z","shell.execute_reply.started":"2024-11-12T18:48:57.456288Z","shell.execute_reply":"2024-11-12T18:48:57.463185Z"}},"outputs":[{"name":"stdout","text":"0.5166666666666667\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Calculate accuracy by checking exact matches between the two arrays\ndef calculate_accuracy(predicted, true_test):\n    # Convert to numpy arrays for easy comparison\n    predicted_np = np.array(predicted)\n    true_test_np = np.array(true_test)\n    \n    # Check for exact matches row by row\n    matches = np.all(predicted_np == true_test_np, axis=1)\n    accuracy = np.mean(matches) * 100  # Percentage of exact matches\n    return accuracy\n\n# Calculate and print the accuracy\naccuracy = calculate_accuracy(true_test_rankings,predicted_rankings_final )\nprint(f\"Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T18:52:04.887641Z","iopub.execute_input":"2024-11-12T18:52:04.888092Z","iopub.status.idle":"2024-11-12T18:52:04.895585Z","shell.execute_reply.started":"2024-11-12T18:52:04.888052Z","shell.execute_reply":"2024-11-12T18:52:04.894584Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 7.14%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}